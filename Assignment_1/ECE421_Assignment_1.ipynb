{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ECE421 Assignment 1",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2OsBajwVHkfk"
      },
      "source": [
        "## Part 1: Linear Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m4dU-37yLmsg",
        "outputId": "3b31b4ac-499c-43c5-c8bd-cb28c06673d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        }
      },
      "source": [
        "## Importing Data\n",
        "# tf.__version__"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-65996cac02ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9unJon-SH9AJ",
        "outputId": "b0f32e2c-6546-4f46-e482-e8a831fc5638",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 62
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def loadData():\n",
        "    with np.load('notMNIST.npz') as data :\n",
        "        Data, Target = data ['images'], data['labels']\n",
        "        posClass = 2\n",
        "        negClass = 9\n",
        "        dataIndx = (Target==posClass) + (Target==negClass)\n",
        "        Data = Data[dataIndx]/255.\n",
        "        Target = Target[dataIndx].reshape(-1, 1)\n",
        "        Target[Target==posClass] = 1\n",
        "        Target[Target==negClass] = 0\n",
        "        np.random.seed(421)\n",
        "        randIndx = np.arange(len(Data))\n",
        "        np.random.shuffle(randIndx)\n",
        "        Data, Target = Data[randIndx], Target[randIndx]\n",
        "        trainData, trainTarget = Data[:3500], Target[:3500]\n",
        "        validData, validTarget = Data[3500:3600], Target[3500:3600]\n",
        "        testData, testTarget = Data[3600:], Target[3600:]\n",
        "    return trainData, validData, testData, trainTarget, validTarget, testTarget"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j9IIp2fGK5_S",
        "outputId": "149e730a-505d-4f5a-b213-b1a5a8490a3f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "source": [
        "trainData, validData, testData, trainTarget, validTarget, testTarget = loadData()\n",
        "print(trainTarget)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0]\n",
            " [1]\n",
            " [1]\n",
            " ...\n",
            " [0]\n",
            " [1]\n",
            " [0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypOQaSU73AeD",
        "outputId": "b8798bb3-66a6-4d03-8d66-bc64ae8b318c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Input Variables\n",
        "#     W: <features_x * features_y, 1>\n",
        "#     b: scaler\n",
        "#     x: <n_samples, features_x * features_y>\n",
        "#     y: <n_samples, 1>\n",
        "#     reg: scaler\n",
        "\n",
        "def reshape_data(data):\n",
        "    '''\n",
        "    Reshapes data x to <n_samples, n_features>\n",
        "    \n",
        "    Input:\n",
        "        Data <n_samples, features_x, features_y>\n",
        "    \n",
        "    Output: \n",
        "        Data_aug <n_samples, features_x * features_y>,\n",
        "        The data matrix, flattened.\n",
        "    '''\n",
        "    \n",
        "    data_aug = np.reshape(data, (data.shape[0], data.shape[1]*data.shape[2]))\n",
        "    return data_aug\n",
        "\n",
        "trainData, validData, testData= map(reshape_data, (trainData, validData, testData))\n",
        "\n",
        "print(trainData.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3500, 784)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFD0pEebMGeO",
        "outputId": "df8d0d67-d1bd-4fbd-9b95-bb6eafe3cc52",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "W = 2 * np.random.random_sample((784, 1))-1\n",
        "b = 0\n",
        "x = trainData\n",
        "y = trainTarget\n",
        "reg = 0.01\n",
        "\n",
        "# USE THIS ONE\n",
        "def MSE(W, b, x, y, reg):\n",
        "    e_in = np.matmul(x, W) + b - y\n",
        "    L_D = np.sum(np.square(np.linalg.norm(e_in))) / x.shape[0]\n",
        "    L_W = np.square(np.linalg.norm(W)) * reg / 2\n",
        "    return L_D + L_W\n",
        "\n",
        "def MSECheck(W, b, x, y, reg):\n",
        "    onesCol = np.ones([x.shape[0], 1])\n",
        "    xTilde = np.concatenate((onesCol, x), axis=1)\n",
        "    wTilde = np.insert(W, 0, b).reshape(-1, 1)\n",
        "    e_in = np.matmul(xTilde, wTilde) - y\n",
        "    L_D = np.square(np.linalg.norm(e_in)) / x.shape[0]\n",
        "    L_W = reg / 2 * np.square(np.linalg.norm(W))\n",
        "    return L_D + L_W\n",
        "\n",
        "def MSECheck2(W, b, x, y, reg):\n",
        "    error = np.matmul(x,W) + b - y\n",
        "    mse = (np.sum(error*error))/((np.shape(y)[0])) + reg/2*np.sum(W*W)\n",
        "    return mse\n",
        "\n",
        "print(MSECheck(W, b, x, y, reg))\n",
        "print(MSECheck2(W, b, x, y, reg))\n",
        "print(MSE(W, b, x, y, reg))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "211.30876015088901\n",
            "211.308760150889\n",
            "211.30876015088901\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ole1xrswmX7T",
        "outputId": "1b1f5ce9-3078-4c1c-f6f1-a4b3cb4fca25",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "source": [
        "W = 2 * np.random.random_sample((784, 1))-1\n",
        "b = 0\n",
        "x = trainData\n",
        "y = trainTarget\n",
        "reg = 0.01\n",
        "\n",
        "# USE THIS ONE\n",
        "def gradMSE(W, b, x, y, reg):\n",
        "    e_in = np.matmul(x, W) + b - y\n",
        "    dL_D_dw = 2 * (np.matmul(np.transpose(x), e_in)) / x.shape[0]\n",
        "    dL_W_dw = 2 * reg * W\n",
        "    dL_D_db = 2 * np.sum(e_in) / x.shape[0]\n",
        "    return dL_D_dw + dL_W_dw, dL_D_db\n",
        "\n",
        "def gradMSECheck(W, b, x, y, reg):\n",
        "    error = np.matmul(x,W) + b - y\n",
        "    grad_mse_W = 2 * (np.matmul(np.transpose(x),error)/(np.shape(y)[0])) + 2*reg*W\n",
        "    grad_mse_b = 2 * ((np.sum(error))/(np.shape(y)[0]))\n",
        "    return grad_mse_W, grad_mse_b\n",
        "\n",
        "print(gradMSE(W, b, x, y.copy(), reg)[0][0:5])\n",
        "print(gradMSECheck(W, b, x, y, reg)[0][0:5])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.33643829]\n",
            " [0.44432914]\n",
            " [0.65383208]\n",
            " [0.89754309]\n",
            " [1.2091064 ]]\n",
            "[[0.33643829]\n",
            " [0.44432914]\n",
            " [0.65383208]\n",
            " [0.89754309]\n",
            " [1.2091064 ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-62Bbs_dIa0G",
        "outputId": "e81ab64b-efe7-47cb-cca4-e1d488a64276",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# def gradMSE(W, b, x, y, reg):\n",
        "    # '''\n",
        "    # Inputs:\n",
        "    #   W <28, 28>\n",
        "    #   b scaler\n",
        "    #   x <3500, 28, 28>\n",
        "    #   y <3500, 1>\n",
        "    #   reg scaler\n",
        "    # My variables:\n",
        "    #   X_aug <N, d+1>\n",
        "    #   w_aug <d+1, 1>\n",
        "    #   y <N, 1>\n",
        "    #   reg scaler\n",
        "    # Output:\n",
        "    #   grad <d+1, 1>\n",
        "    #     where grad[i] is dL/dwi\n",
        "    # '''  \n",
        "    # # Your implementation here\n",
        "    # onesCol = np.ones([x.shape[0], 1])\n",
        "    # X_aug = np.concatenate((onesCol, x), axis=1) \n",
        "    # w_aug = np.insert(W.flatten(), 0, b)\n",
        "\n",
        "    # dL = gradMSEHelper(X_aug, w_aug, y, reg)\n",
        "    # return dL\n",
        "\n",
        "def gradMSEHelper(X_aug, w_aug, y, reg):\n",
        "    # L = L_loss + L_reg\n",
        "    # dL_loss = 2/N (transpose(X_aug) * (X_aug * w_aug - y)\n",
        "    N = X_aug.shape[0]\n",
        "    dL_loss = 2/N * np.matmul(np.transpose(X_aug), np.matmul(X_aug, w_aug) - y[:, 0])\n",
        "    dL_reg = reg * w_aug\n",
        "    dL_reg[0] = 0\n",
        "    dL = dL_loss + dL_reg\n",
        "    return dL"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rm9uhxSsBt14"
      },
      "source": [
        "def grad_descent(W, b, x, y, alpha, epochs, reg, error_tol=10**(-7), lossType=\"MSE\"):\n",
        "    if (lossType=='CE'):\n",
        "        for _ in range(epochs):\n",
        "            dW, db = gradCE(W, b, x, y, reg)\n",
        "            W = W - alpha*dW\n",
        "            b = b - alpha*db\n",
        "            if (np.linalg.norm(alpha*dW)<=error_tol):\n",
        "                break\n",
        "            loss.append(crossEntropyLoss(W, b, x, y, reg))\n",
        "    elif (lossType == \"MSE\"):\n",
        "        for i in range(epochs):\n",
        "            dW, db = gradMSE(W, b, x, y, reg)\n",
        "            W = W - alpha * dW\n",
        "            b = b - alpha * db\n",
        "            if (np.linalg.norm(alpha * dW) <= error_tol):\n",
        "                break\n",
        "            loss.append(MSE(W, b, x, y, reg))\n",
        "      return W, b"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fN6vWAg2Alqc",
        "outputId": "03d5bc4f-4379-44ff-cda2-297cc914a9ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "W = 2*np.random.random_sample((784, 1))-1\n",
        "b = 0\n",
        "trainLoss = []\n",
        "validLoss = []\n",
        "testLoss = []\n",
        "trainAccuracy = []\n",
        "validAccuracy = []\n",
        "testAccuracy = []\n",
        "xTrain = trainData\n",
        "xValid = validData\n",
        "xTest = testData\n",
        "yTrain = trainTarget\n",
        "yValid = validTarget\n",
        "yTest = testTarget\n",
        "reg = 0.01\n",
        "\n",
        "def grad_descent(W, b, x, y, alpha, epochs, reg, error_tol=10**(-7), lossType=\"MSE\"):\n",
        "    if (lossType=='CE'):\n",
        "        for _ in range(epochs):\n",
        "            dW, db = gradCE(W, b, x, y, reg)\n",
        "            W = W - alpha*dW\n",
        "            b = b - alpha*db\n",
        "            if (np.linalg.norm(alpha*dW)<=error_tol):\n",
        "                break\n",
        "            loss.append(crossEntropyLoss(W, b, x, y, reg))\n",
        "    elif (lossType == \"MSE\"):\n",
        "        for i in range(epochs):\n",
        "            dW, db = gradMSE(W, b, xTrain, yTrain, reg)\n",
        "            W = W - alpha * dW\n",
        "            b = b - alpha * db\n",
        "            if (np.linalg.norm(alpha * dW) <= error_tol):\n",
        "                break\n",
        "            trainLoss.append(MSE(W, b, x, y, reg))\n",
        "            validLoss.append(MSE(W, b, xValid, yValid, reg))\n",
        "            testLoss.append(MSE(W, b, xTest, yTest, reg))\n",
        "            yHatTrain = np.matmul(x, W) + b\n",
        "            yHatValid = np.matmul(xValid, W) + b\n",
        "            yHatTest = np.matmul(xTest, W) + b\n",
        "            for i in range(len(yHatTrain)):\n",
        "              if yHatTrain[i] >= 0.5:\n",
        "                yHatTrain[i] = 1\n",
        "              else:\n",
        "                yHatTrain[i] = 0\n",
        "            for i in range(len(yHatValid)):\n",
        "              if yHatValid[i] >= 0.5:\n",
        "                yHatValid[i] = 1\n",
        "              else:\n",
        "                yHatValid[i] = 0\n",
        "            for i in range(len(yHatTest)):\n",
        "              if yHatTest[i] >= 0.5:\n",
        "                yHatTest[i] = 1\n",
        "              else:\n",
        "                yHatTest[i] = 0\n",
        "            trainSuccesses = 0\n",
        "            validSuccesses = 0\n",
        "            testSuccesses = 0\n",
        "            for i in range(len(yHatTrain)):\n",
        "              if yHatTrain[i] == yTrain[i]:\n",
        "                trainSuccesses += 1\n",
        "            for i in range(len(yHatValid)):\n",
        "              if yHatValid[i] == yValid[i]:\n",
        "                validSuccesses += 1\n",
        "            for i in range(len(yHatTest)):\n",
        "              if yHatTest[i] == yTest[i]:\n",
        "                testSuccesses += 1\n",
        "            trainAccuracy.append(float(trainSuccesses)/len(yHatTrain))\n",
        "            validAccuracy.append(float(validSuccesses)/len(yHatValid))\n",
        "            testAccuracy.append(float(testSuccesses)/len(yHatTest))\n",
        "    return W, b\n",
        "\n",
        "wFit, bFit = grad_descent(W, b, xTrain, yTrain, alpha=0.005, epochs = 5000, reg = 0.1)\n",
        "\n",
        "print(MSE(wFit, bFit, xTrain, yTrain, 0))\n",
        "\n",
        "t = np.arange(len(trainLoss))\n",
        "\n",
        "fig, axes = plt.subplots(1, 2)\n",
        "plt.suptitle('Alpha = 0.005, Lambda = 0.1')\n",
        "axes[0].plot(t, trainLoss)\n",
        "axes[0].plot(t, validLoss)\n",
        "axes[0].plot(t, testLoss)\n",
        "axes[1].plot(t, trainAccuracy)\n",
        "axes[1].plot(t, validAccuracy)\n",
        "axes[1].plot(t, testAccuracy)\n",
        "\n",
        "axes[0].legend(['Training', 'Validation', 'Test'], loc='upper right')\n",
        "axes[1].legend(['Training', 'Validation', 'Test'], loc='lower right')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.39006643163948773\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LenL641Kvtdi",
        "outputId": "67dcbec1-01df-4c30-f16a-1536f434d0da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "xTrain = trainData\n",
        "xValid = validData\n",
        "xTest = testData\n",
        "yTrain = trainTarget\n",
        "yValid = validTarget\n",
        "yTest = testTarget\n",
        "\n",
        "onesCol = np.ones([xTrain.shape[0], 1])\n",
        "xTilde = np.concatenate((onesCol, xTrain), axis=1)\n",
        "wStar = np.matmul(np.linalg.inv(np.matmul(np.transpose(xTilde), xTilde)), np.matmul(np.transpose(xTilde), y))\n",
        "yHatTrain = np.matmul(xTrain, wStar[1:]) + wStar[0]\n",
        "yHatValid = np.matmul(xValid, wStar[1:]) + wStar[0]\n",
        "yHatTest = np.matmul(xTest, wStar[1:]) + wStar[0]\n",
        "for i in range(len(yHatTrain)):\n",
        "  if yHatTrain[i] >= 0.5:\n",
        "    yHatTrain[i] = 1\n",
        "  else:\n",
        "    yHatTrain[i] = 0\n",
        "for i in range(len(yHatValid)):\n",
        "  if yHatValid[i] >= 0.5:\n",
        "    yHatValid[i] = 1\n",
        "  else:\n",
        "    yHatValid[i] = 0\n",
        "for i in range(len(yHatTest)):\n",
        "  if yHatTest[i] >= 0.5:\n",
        "    yHatTest[i] = 1\n",
        "  else:\n",
        "    yHatTest[i] = 0\n",
        "trainSuccesses = 0\n",
        "validSuccesses = 0\n",
        "testSuccesses = 0\n",
        "for i in range(len(yHatTrain)):\n",
        "  if yHatTrain[i] == yTrain[i]:\n",
        "    trainSuccesses += 1\n",
        "for i in range(len(yHatValid)):\n",
        "  if yHatValid[i] == yValid[i]:\n",
        "    validSuccesses += 1\n",
        "for i in range(len(yHatTest)):\n",
        "  if yHatTest[i] == yTest[i]:\n",
        "    testSuccesses += 1\n",
        "print(float(trainSuccesses)/len(yHatTrain))\n",
        "print(float(validSuccesses)/len(yHatValid))\n",
        "print(float(testSuccesses)/len(yHatTest))\n",
        "\n",
        "print(MSE(wStar[1:], wStar[0], xTrain, yTrain, 0))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9937142857142857\n",
            "0.96\n",
            "0.9448275862068966\n",
            "0.01870405565313653\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99DCEkFFIiZt"
      },
      "source": [
        "def crossEntropyLoss(W, b, x, y, reg):\n",
        "    # Your implementation here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNSh_vebIlA8"
      },
      "source": [
        "def gradCE(W, b, x, y, reg):\n",
        "    # Your implementation here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1xiiHv_XInE9"
      },
      "source": [
        "def buildGraph(loss=\"MSE\"):\n",
        "\t#Initialize weight and bias tensors\n",
        "\ttf.set_random_seed(421)\n",
        "\n",
        "\tif loss == \"MSE\":\n",
        "\t# Your implementation\n",
        "\t\n",
        "\telif loss == \"CE\":\n",
        "\t#Your implementation here"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}